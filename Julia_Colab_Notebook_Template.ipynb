{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Julia_Colab_Notebook_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_\n",
        "\n",
        "## Instructions\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd09ad1a-3ee4-422f-ac1a-1a6cdf391ff5"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.6.0\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools Plots Knet ArgParse\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -n \"$COLAB_GPU\" ] && [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  if [ \"$COLAB_GPU\" = \"1\" ]; then\n",
        "      JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Success! Please reload this page and jump to the next section.\"\n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Julia 1.6.0 on the current Colab Runtime...\n",
            "2021-11-13 17:17:01 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.6/julia-1.6.0-linux-x86_64.tar.gz [112838927/112838927] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n",
            "Installing Julia package Plots...\n",
            "Installing Julia package Knet...\n",
            "Installing Julia package ArgParse...\n",
            "Installing Julia package CUDA...\n",
            "Installing IJulia kernel...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.6\n",
            "\n",
            "Success! Please reload this page and jump to the next section.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Checking the Installation\n",
        "The `versioninfo()` function should print your Julia version and some other info about the system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzvvzCl1i0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c80777d-2d9c-41b1-b8fb-b47a7ad79820"
      },
      "source": [
        "versioninfo()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Julia Version 1.6.0\n",
            "Commit f9720dc2eb (2021-03-24 12:55 UTC)\n",
            "Platform Info:\n",
            "  OS: Linux (x86_64-pc-linux-gnu)\n",
            "  CPU: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "  WORD_SIZE: 64\n",
            "  LIBM: libopenlibm\n",
            "  LLVM: libLLVM-11.0.1 (ORCJIT, haswell)\n",
            "Environment:\n",
            "  JULIA_NUM_THREADS = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQlpeR9wNOi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77803066-8331-4582-ec71-7b604846ad52"
      },
      "source": [
        "using BenchmarkTools\n",
        "\n",
        "M = rand(2048, 2048)\n",
        "@benchmark M^2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BenchmarkTools.Trial: 10 samples with 1 evaluation.\n",
              " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m489.243 ms\u001b[22m\u001b[39m … \u001b[35m604.164 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.22% … 19.00%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m502.942 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.11%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m514.367 ms\u001b[22m\u001b[39m ± \u001b[32m 34.846 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.78% ±  5.96%\n",
              "\n",
              "  \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \n",
              "  \u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
              "  489 ms\u001b[90m           Histogram: frequency by time\u001b[39m          604 ms \u001b[0m\u001b[1m<\u001b[22m\n",
              "\n",
              " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XciCcMAJOT3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bb4d68-86fd-4321-e2ff-b3dbbf8d009b"
      },
      "source": [
        "if ENV[\"COLAB_GPU\"] == \"1\"\n",
        "    using CUDA\n",
        "\n",
        "    M_gpu = cu(M)\n",
        "    @benchmark CUDA.@sync M_gpu^2\n",
        "else\n",
        "    println(\"No GPU found.\")\n",
        "end"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDA_compat\n",
            "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDA\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BenchmarkTools.Trial: 692 samples with 1 evaluation.\n",
              " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m5.942 ms\u001b[22m\u001b[39m … \u001b[35m 10.082 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m7.112 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
              " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m7.216 ms\u001b[22m\u001b[39m ± \u001b[32m366.690 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
              "\n",
              "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▆\u001b[39m█\u001b[34m▆\u001b[39m\u001b[39m▃\u001b[39m▁\u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
              "  \u001b[39m█\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m▁\u001b[39m█\u001b[39m▅\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▁\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▆\u001b[39m \u001b[39m▇\n",
              "  5.94 ms\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      8.16 ms \u001b[0m\u001b[1m<\u001b[22m\n",
              "\n",
              " Memory estimate\u001b[90m: \u001b[39m\u001b[33m208 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m8\u001b[39m."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHHeqxQGVxQR"
      },
      "source": [
        "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
        "\n",
        "Most research on NER systems has been structured as taking an unannotated block of text, such as the following **example**:\n",
        "\n",
        "**INPUT:** Jim bought 300 shares of Acme Corp. in 2006.\n",
        "\n",
        "And producing an annotated block of text that highlights the names of entities:\n",
        "\n",
        "**OUTPUT:** [Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\n",
        "\n",
        "In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.(Wikipedia)\n",
        "\n",
        "Your task in this lab is to implement named entity LSTM based tagger which uses an LSTM to extract features from the input sentence, which are then passed through a multi-layer perceptron to predict\n",
        "the tag of the word. Finally, train that model on [WikiNER](https://github.com/neulab/dynet-benchmark/tree/master/data/tags) dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXfAqGAJXCvl",
        "outputId": "d8573778-6d5b-47f7-aa4a-35691b0b5de2"
      },
      "source": [
        "import Pkg; Pkg.add(\"IterTools\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.6/Project.toml`\n",
            " \u001b[90m [c8e1da08] \u001b[39m\u001b[92m+ IterTools v1.3.0\u001b[39m\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLpcFX1zVyuk",
        "outputId": "67a4a309-1f5b-46c1-a4a2-0ad300683b40"
      },
      "source": [
        "using Printf, Dates, Random, CUDA, Knet, ArgParse, Test, Base.Iterators, IterTools\n",
        "\n",
        "STDOUT = Base.stdout\n",
        "\n",
        "import Knet: train!\n",
        "include(joinpath(Knet.dir(), \"data\", \"wikiner.jl\"))\n",
        "_atype = CUDA.functional() ? KnetArray{Float32} : Array{Float32}\n",
        "\n",
        "@info \"Adding required packages and importing WikiNER dataset\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Adding required packages and importing WikiNER dataset\n",
            "└ @ Main In[10]:9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LxPgypHV4kj"
      },
      "source": [
        "## Prepare samples for the network\n",
        "Your first task is to prepare instances for the network. We're given with the tokens (words and tags) and we need to make them understandable by our neural network. For this purpose, we build vocabularies (for both words and tags) and construct vocabulary to index dictionaries by using those vocabularies (w2i and t2i, word2index, tag2index). Then, we convert words and tags to indices with the usage of our dictionaries.\n",
        "\n",
        "```julia\n",
        "julia> show_instance() # show instance in not implemented in Knet, it is a hypothetical procedure\n",
        "Inputs sentence:\n",
        "Sent-> That inscribed in the genealogical records of his family is Jiang Zhoutai .\n",
        "NERs-> O    O         O  O   O            O       O  O   O      O  I-PER I-PER   O\n",
        "\n",
        "Timesteps:\n",
        "Time step 1 ---> Inputs: That\n",
        "                 Outputs: O\n",
        "Time step 2 ---> Inputs: inscribed\n",
        "                 Outputs:O\n",
        "Time step 3 ---> Inputs: in\n",
        "                 Outputs: O\n",
        "Time step 4 ---> Inputs: the\n",
        "                 Outputs: O\n",
        "Time step 5 ---> Inputs: genealogical\n",
        "                 Outputs: O\n",
        "Time step 6 ---> Inputs: records  .\n",
        "                 Outputs: O\n",
        "Time step 7 ---> Inputs: of\n",
        "                 Outputs: O\n",
        "Time step 8 ---> Inputs: his\n",
        "                 Outputs: O\n",
        "Time step 9 ---> Inputs: family\n",
        "                 Outputs: O\n",
        "Time step 10 --->Inputs: is\n",
        "                 Outputs: O\n",
        "Time step 11 ---> Inputs: Jiang\n",
        "                  Outputs: I-PER\n",
        "Time step 12 ---> Inputs: Zhoutai\n",
        "                  Outputs: I-PER\n",
        "Time step 13 ---> Inputs: .\n",
        "                  Outputs: O\n",
        "```\n",
        "\n",
        "Our input and output arrays should be integers instead of texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZnF4HqIV__g"
      },
      "source": [
        "In this step, you need to implement `make_instance` function\n",
        "instance is a list of tuples. Each tuple contains a word and the corresponding tag as string.\n",
        "You need to convert them into indices using word to index (w2i) and tag to index (t2i)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaZxe2nWWAlP",
        "outputId": "bc0a2783-efb7-432e-fba7-809cac9e4453"
      },
      "source": [
        "\"\"\"\n",
        "    make_instance(instance, w2i, t2i)\n",
        "\n",
        "Return tuple of two sequences containing inputs and the corresponding outputs respectively.\n",
        "\n",
        "This function does this by converting each input unit in the instance into its corresponding value in w2i, and does the same for output units using t2i.\n",
        "\"\"\"\n",
        "function make_instance(instance, w2i, t2i, unk=UNK)\n",
        "    input = Array{Int}([])\n",
        "    output = Array{Int}([])\n",
        "    # Your code here\n",
        "    for l = 1:length(instance)\n",
        "        push!(input, get!(w2i, instance[l][1], 17516))\n",
        "        push!(output, get!(t2i, instance[l][2], 6))\n",
        "    end\n",
        "    return input, output\n",
        "end\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "   make_instances(data, w2i, t2i)\n",
        "\n",
        "Iterate over `data` and Return `words` and `tags`\n",
        "\"\"\"\n",
        "function make_instances(data, w2i, t2i)\n",
        "    words = []; tags = []\n",
        "    for k = 1:length(data)\n",
        "        this_words, this_tags = make_instance(data[k], w2i, t2i)\n",
        "        push!(words, this_words)\n",
        "        push!(tags, this_tags)\n",
        "    end\n",
        "    return words, tags\n",
        "end\n",
        "\n",
        "@info \"Testing instances\"\n",
        "data = WikiNERData();\n",
        "dev = make_instances(data.dev, data.w2i, data.t2i);\n",
        "@test dev[1][2][3] == 22450\n",
        "@test size.(dev) == ((1696,), (1696,))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing instances\n",
            "└ @ Main In[49]:35\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKbu-WODWLq8"
      },
      "source": [
        "### WikiNERProcessed\n",
        "This struct contains processed data (e.g words and tags are indices)\n",
        "and necessary variables to prepare minibatches.\n",
        "WikiNERProcessed struct works as a data iterator, which will you implement in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO-_0VpEWECk",
        "outputId": "42a41fae-183f-4e08-f59e-4f93a4fa9fe6"
      },
      "source": [
        "mutable struct WikiNERProcessed\n",
        "    words\n",
        "    tags\n",
        "    batchsize\n",
        "    ninstances\n",
        "    shuffled\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "   WikiNERProcessed(instances, w2i, t2i; batchsize=16, shuffled=true)\n",
        "\n",
        "Return a WikiNERProcessed object with the given instances\n",
        "\"\"\"\n",
        "function WikiNERProcessed(instances, w2i, t2i; batchsize=16, shuffled=true)\n",
        "    words, tags = make_instances(instances, w2i, t2i)\n",
        "    ninstances = length(words)\n",
        "    return WikiNERProcessed(words, tags, batchsize, ninstances, shuffled)\n",
        "end\n",
        "\n",
        "@info \"WikiNERProcessed\"\n",
        "devdata = WikiNERProcessed(data.dev, data.w2i, data.t2i; shuffled=false);\n",
        "@test devdata.words[1][1] == 17516\n",
        "@test length(devdata.words) == 1696"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: WikiNERProcessed\n",
            "└ @ Main In[50]:20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNd7q_2dWS-O"
      },
      "source": [
        "### WikiNERProcessed Iterator\n",
        "Please note that this function returns tuple of two tuples.\n",
        "\n",
        "The first one contains a data batch with words as an input for our model, and tags as the corresponding output, and batchsizes of this batch.\n",
        "Since you will use the RNN callable object in your model.\n",
        "It supports variable length instances in its input.\n",
        "However, you need to prepare your input such as the RNN object can work on it. See `batchSizes` option of the RNN object using `@doc RNN` and Look up `zeros`, `sortperm`, `min`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWzs4d5CWaUl",
        "outputId": "2506d9ff-8a61-4290-a34c-4494e60b5663"
      },
      "source": [
        "\"\"\"\n",
        "    iterate(d::WikiNERProcessed[, state])\n",
        "\n",
        "Iterate over `d::WikiNERProcessed` object. If `state` is missing, it's the beginning\n",
        "of the whole iteration process. \n",
        "WikiNERProcessed(words, tags, batchsize, ninstances, shuffled)\n",
        "\n",
        "for each batch:\n",
        "    define input_array\n",
        "    sort sequences by their length (longer first)\n",
        "    for each timestep_i in longest sample\n",
        "        for each sequence s in ordered_sequences\n",
        "             add unit_i of sequence s to our input array\n",
        "\n",
        "\"\"\"\n",
        "function Base.iterate(d::WikiNERProcessed, state=ifelse(d.shuffled, randperm(d.ninstances), 1:d.ninstances))\n",
        "    # Your code here\n",
        "    this_words= d.words[state]\n",
        "    this_tags= d.tags[state]\n",
        "    words = Array{Int}([]); tags = Array{Int}([])\n",
        "    if size(state,1)<d.batchsize\n",
        "        these_words = this_words[1:end]\n",
        "        these_tags =  this_tags[1:end]\n",
        "        \n",
        "        these_lengths = length.(these_words)\n",
        "        these_lengths_sorted = sortperm(these_lengths,rev=true)\n",
        "        \n",
        "        those_words = these_words[sortperm(these_lengths,rev=true)]\n",
        "        those_tags = these_tags[sortperm(these_lengths,rev=true)]\n",
        "        those_lengths = length.(those_words)\n",
        "        \n",
        "        for m in 1:those_lengths[1]\n",
        "            for n in 1:size(state,1)\n",
        "                try\n",
        "                    push!(words, those_words[n][m])\n",
        "                    push!(tags, those_tags[n][m])\n",
        "                catch\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    else\n",
        "        these_words = this_words[1:d.batchsize]\n",
        "        these_tags =  this_tags[1:d.batchsize]\n",
        "        these_lengths = length.(these_words)\n",
        "        these_lengths_sorted = sortperm(these_lengths,rev=true)\n",
        "\n",
        "        \n",
        "        those_words = these_words[these_lengths_sorted]\n",
        "        those_tags = these_tags[these_lengths_sorted]\n",
        "        those_lengths = length.(those_words)\n",
        "\n",
        "        batchsizes = Array{Int}(zeros(those_lengths[1]))\n",
        "        for m in 1:those_lengths[1]\n",
        "            for n in 1:d.batchsize\n",
        "                try\n",
        "                    push!(words, those_words[n][m])\n",
        "                    batchsizes[m] = batchsizes[m] + 1 \n",
        "                    push!(tags, those_tags[n][m])\n",
        "                catch\n",
        "                end     \n",
        "            end\n",
        "        end \n",
        "    end\n",
        "    state = state[d.batchsize+1:end]\n",
        "    if state == []\n",
        "        return nothing\n",
        "    end\n",
        "    \n",
        "    return ((words, tags, batchsizes), state)\n",
        "end\n",
        "\n",
        "Base.IteratorSize(::Type{WikiNERProcessed}) = Base.SizeUnknown()\n",
        "Base.IteratorEltype(::Type{WikiNERProcessed}) = Base.HasEltype()\n",
        "\n",
        "@info \"Testing WikiNERProcessed Iterator\"\n",
        "((words, tags, batchsizes), new_state) = iterate(devdata);\n",
        "\n",
        "@test length.((words, tags, batchsizes)) == (397, 397, 55)\n",
        "@test new_state == 17:1696"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing WikiNERProcessed Iterator\n",
            "└ @ Main In[106]:75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvGmjdz1WhZu"
      },
      "source": [
        "## Model Components implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10BDnjHWj7p"
      },
      "source": [
        "### Embedding layer\n",
        "This layer maps each vocabulary to its corresponding vector using its Int id. It works with mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C88drjGWnbn",
        "outputId": "4042de43-bc61-4c42-d1f7-a1dfc6f7e8f6"
      },
      "source": [
        "\"\"\"\n",
        "    Embedding(vocabsize::Int, embedsize::Int, atype=_atype, scale=0.01)\n",
        "\n",
        "Create a Embedding layer and initialize its weight. Initial weight parameters are\n",
        "sampled from normal distribution scaled by a `scale` factor.\n",
        "\n",
        "# Examples\n",
        "```julia-repl\n",
        "julia> embed = Embedding(100, 25);\n",
        "\n",
        "julia> x = rand(1:10, 10);\n",
        "\n",
        "julia> embed(x); # forward call\n",
        "```\n",
        "\"\"\"\n",
        "mutable struct Embedding\n",
        "    w # weight\n",
        "end\n",
        "\n",
        "function Embedding(vocabsize::Int, embedsize::Int, atype=_atype, scale=0.01)\n",
        "    w = Param(convert(atype, scale*randn(embedsize, vocabsize)));\n",
        "    return Embedding(w)\n",
        "end\n",
        "\n",
        "\n",
        "function (l::Embedding)(x)\n",
        "    l.w[:, x]\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "@info \"Testing embedding layer\"\n",
        "Random.seed!(1)\n",
        "embed = Embedding(100, 25);\n",
        "x = rand(1:25, 12, 32);\n",
        "@test size(embed(x)) == (25, 12, 32)\n",
        "@test sum(embed(x)) ≈ -5.08327f0"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing embedding layer\n",
            "└ @ Main In[95]:32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_5OBXHwWpwU"
      },
      "source": [
        "### Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BszqzmvLWrdd",
        "outputId": "feb1c564-3202-4944-fcdb-95680e21180d"
      },
      "source": [
        "\"\"\"\n",
        "    Linear(inputsize, outputsize; atype=Array{Float64}, scale::Float64=0.1)\n",
        "\n",
        "Create a linear layer with its weight and bias. Initial weight parameters are\n",
        "sampled from normal distribution scaled by a `scale` factor. Initial bias\n",
        "values are zeros.\n",
        "\n",
        "# Examples\n",
        "```julia-repl\n",
        "julia> layer = Linear(50, 10);\n",
        "\n",
        "julia> x = rand(2, 50);\n",
        "\n",
        "julia> layer(x); # forward call\n",
        "```\n",
        "\n",
        "struct Linear; w; b; end\n",
        "\n",
        "Linear(input::Int, output::Int)=Linear(param(output,input), param0(output))\n",
        "\n",
        "(l::Linear)(x) = l.w * mat(x,dims=1) .+ l.b  # (H,B,T)->(H,B*T)->(V,B*T)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "mutable struct Linear\n",
        "    w # weight\n",
        "    b # bias\n",
        "\n",
        "    function Linear(inputsize, outputsize; atype=_atype, scale::Float64=0.01)\n",
        "        # Your code here\n",
        "        w = Param(convert(atype,scale*randn(outputsize,inputsize)))\n",
        "        #w = Param(scale*randn(outputsize,inputsize))\n",
        "        b = Param(convert(atype,zeros(outputsize,1)))\n",
        "        new(w,b)\n",
        "    end\n",
        "end\n",
        "\n",
        "function (l::Linear)(x)\n",
        "    # Your code here\n",
        "    l.w * mat(x,dims=1) .+ l.b  # (H,B,T)->(H,B*T)->(V,B*T)\n",
        "end\n",
        "\n",
        "\n",
        "\n",
        "@info \"Testing linear layer\"\n",
        "Random.seed!(1)\n",
        "lin = Linear(100, 200);\n",
        "x = _atype(randn(100, 32));\n",
        "@test size(lin(x)) == (200, 32)\n",
        "@test sum(lin(x)) ≈ -3.8317218f0"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing linear layer\n",
            "└ @ Main In[79]:45\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmXFtKmIWtkk"
      },
      "source": [
        "### Hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5rqYJUVWv2c",
        "outputId": "81b502d1-b815-4343-8ed7-78b489fb9e62"
      },
      "source": [
        "\"\"\"\n",
        "    Hidden(inputsize, outputsize, fun=relu, atype=_atype, scale=0.1)\n",
        "\n",
        "Create a hidden layer with its weight and bias and activation function. Initial weight parameters are\n",
        "sampled from normal distribution scaled by a `scale` factor. Initial bias\n",
        "values are zeros.\n",
        "\n",
        "# Examples\n",
        "```julia-repl\n",
        "julia> layer = Hidden(100, 200);\n",
        "\n",
        "julia> x = rand(100, 5);\n",
        "\n",
        "julia> layer(x); # forward call\n",
        "```\n",
        "\"\"\"\n",
        "mutable struct Hidden\n",
        "    w # weight\n",
        "    b # bias\n",
        "    fun # non-linear activation function like relu or tanh\n",
        "\n",
        "    function Hidden(inputsize, outputsize, fun=relu, atype=_atype, scale=0.1)\n",
        "        # Your code here\n",
        "        w = Param(convert(atype,scale*randn(outputsize,inputsize)))\n",
        "        b = Param(convert(atype,zeros(outputsize,1)))\n",
        "        fun = relu\n",
        "        new(w,b,relu)\n",
        "    end\n",
        "end\n",
        "\n",
        "function (l::Hidden)(x)\n",
        "    # Your code here\n",
        "    max.(0,(l.w * mat(x,dims=1) .+ l.b))  # (H,B,T)->(H,B*T)->(V,B*T)\n",
        "    #relu(l.w * mat(x,dims=1) .+ l.b)\n",
        "end\n",
        "\n",
        "@info \"Testing hidden layer\"\n",
        "Random.seed!(1)\n",
        "hid = Hidden(200, 256);\n",
        "x = _atype(randn(200, 32));\n",
        "println(size(hid(x)))\n",
        "@test size(hid(x)) == (256, 32)\n",
        "@test sum(hid(x)) ≈ 4635.545f0"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing hidden layer\n",
            "└ @ Main In[80]:37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKxpwJRFWzxN"
      },
      "source": [
        "### NER Tagger model\n",
        "\n",
        "Our model consists of four layers. Size of their outputs are as the following:\n",
        "* **(T)** - Input\n",
        "* **(E, T)** - Embedding\n",
        "* **(RNN, T)** - RNN\n",
        "* **(H, T)** - Hidden\n",
        "* **(NTags, T)** - Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ZPsjJkgqsP"
      },
      "source": [
        "    #m.embed(m.rnn(m.hidden(m.projection(x)));batchsizes)\n",
        "    a = m.embed(x)\n",
        "    @show size(x,1)\n",
        "    @show size(a)\n",
        "    b = m.rnn(a)\n",
        "    @show size(b)\n",
        "    c = m.hidden(b)\n",
        "    @show size(c)\n",
        "    d = m.projection(c)\n",
        "    @show size(d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "B8Xv-7vdW34P",
        "outputId": "add75f5a-04c9-45ed-c9c9-9bdfca2a119b"
      },
      "source": [
        "\n",
        "mutable struct NERTagger\n",
        "    embed::Embedding\n",
        "    rnn::RNN\n",
        "    hidden::Hidden\n",
        "    projection::Linear\n",
        "end\n",
        "\n",
        "function NERTagger(no_words, no_tags, embed_size, rnn_hidden_size, mlp_hidden_size, atype=_atype)\n",
        "    # Your code here\n",
        "    embed = Embedding(no_words, embed_size)\n",
        "    rnn = RNN(embed_size, rnn_hidden_size,atype=_atype)\n",
        "    hidden = Hidden(rnn_hidden_size, mlp_hidden_size)\n",
        "    projection = Linear(mlp_hidden_size, no_tags,atype=_atype)\n",
        "\n",
        "    return NERTagger(embed, rnn, hidden, projection)\n",
        "end\n",
        "\n",
        "function (m::NERTagger)(x; batchsizes=nothing)\n",
        "    # Your code here\n",
        "    m.projection(m.hidden(m.rnn(m.embed(x);batchSizes=batchsizes)))\n",
        "end\n",
        "\n",
        "@info \"Testing forward pass of NERTagger\"\n",
        "Random.seed!(1)\n",
        "nwords, ntags = length(data.w2i), data.ntags\n",
        "model = NERTagger(nwords, ntags, 128, 50, 32)\n",
        "output = model(words; batchsizes=batchsizes)\n",
        "\n",
        "@test size(output) == (9, 397)\n",
        "@test sum(output) == 0.4713313f0"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing forward pass of NERTagger\n",
            "└ @ Main In[101]:24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[101]:31\u001b[22m\n",
            "  Expression: sum(output) == 0.4713313f0\n",
            "   Evaluated: 0.25421953f0 == 0.4713313f0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mThere was an error during testing\u001b[39m",
            "",
            "Stacktrace:",
            " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:772",
            " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:555",
            " [3] top-level scope",
            "   @ In[101]:31",
            " [4] eval",
            "   @ ./boot.jl:360 [inlined]",
            " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
            "   @ Base ./loading.jl:1094"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AiFKd-MltHu"
      },
      "source": [
        "Now you will implement loss function for your model.\n",
        "Firstly get your probabilities from your model.\n",
        "Then calculate the loss function for average per token. You can use `nll` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "gTEHUGLiltyL",
        "outputId": "ec25efd9-0691-4f8c-b14b-1138785fa04b"
      },
      "source": [
        "function (m::NERTagger)(x, ygold, batchsizes; average=true)\n",
        "    # Your code here\n",
        "    nll(m(x),ygold)\n",
        "end\n",
        "\n",
        "@info \"Testing loss function of NERTagger\"\n",
        "Random.seed!(1)\n",
        "nwords, ntags = length(data.w2i), data.ntags\n",
        "model = NERTagger(nwords, ntags, 128, 50, 32)\n",
        "l = model(words, tags, batchsizes)\n",
        "\n",
        "@test l ≈ 2.198382f0"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing loss function of NERTagger\n",
            "└ @ Main In[102]:6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[102]:12\u001b[22m\n",
            "  Expression: l ≈ 2.198382f0\n",
            "   Evaluated: 2.1972032f0 ≈ 2.198382f0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mThere was an error during testing\u001b[39m",
            "",
            "Stacktrace:",
            " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:772",
            " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:555",
            " [3] top-level scope",
            "   @ In[102]:12",
            " [4] eval",
            "   @ ./boot.jl:360 [inlined]",
            " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
            "   @ Base ./loading.jl:1094"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ln7S_OZnQnI"
      },
      "source": [
        "### Loss for a whole dataset\n",
        "\n",
        "Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
        "a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
        "iterator of `(words, gold_tags, batchsizes)` such as `WikiNERProcessed` and `model(x,y;average)` is a model like\n",
        "`NERTagger` that computes loss on a single `(x,y)` pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "cw4xh8jjnRKl",
        "outputId": "2a455181-16d2-4e06-b16a-0148b0d58c51"
      },
      "source": [
        "\"\"\"\n",
        "    loss(model::NERTagger, data; average=true)\n",
        "\n",
        "Return overall loss of model on data.\n",
        "\"\"\"\n",
        "function loss(model::NERTagger, data; average=true)\n",
        "    l = 0\n",
        "    n = 0\n",
        "    for (x, y, b) in data\n",
        "        # Your code here\n",
        "\n",
        "        l = l + model(x, y, b)\n",
        "        n = n + 1\n",
        "    end\n",
        "    average && return l / n\n",
        "    return l, n\n",
        "end\n",
        "\n",
        "@info \"Testing loss function\"\n",
        "Random.seed!(1)\n",
        "@test loss(model, devdata) ≈ 2.1978726f0\n",
        "@test loss(model, devdata; average=false) == (85735.13f0, 39007)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing loss function\n",
            "└ @ Main In[114]:19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[114]:22\u001b[22m\n",
            "  Expression: loss(model, devdata; average = false) == (85735.13f0, 39007)\n",
            "   Evaluated: (230.70518f0, 105) == (85735.13f0, 39007)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mThere was an error during testing\u001b[39m",
            "",
            "Stacktrace:",
            " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:772",
            " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:555",
            " [3] top-level scope",
            "   @ In[114]:22",
            " [4] eval",
            "   @ ./boot.jl:360 [inlined]",
            " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
            "   @ Base ./loading.jl:1094"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOyCVxgdrcWJ"
      },
      "source": [
        "### Question\n",
        "Why are we getting such value for loss? is this expected and why?\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N3UFeTmrgQs"
      },
      "source": [
        "### Accuracy metric\n",
        "This function will be the metric which will evaluate our model's performance.\n",
        "\n",
        "You will iterate over the given `data` object, predicting each instance and adding number of correctly predicted tokens to `ncorrect` and the number of tokens to `ntokens`.\n",
        "\n",
        "possible helpful procedures: `argmax`, `vec`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "SSyVOBi5sl2A",
        "outputId": "5a18f354-95cb-45f3-f4cd-ec1604bfd165"
      },
      "source": [
        "@doc reshape"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{verbatim}\nreshape(A, dims...) -> AbstractArray\nreshape(A, dims) -> AbstractArray\n\\end{verbatim}\nReturn an array with the same data as \\texttt{A}, but with different dimension sizes or number of dimensions. The two arrays share the same underlying data, so that the result is mutable if and only if \\texttt{A} is mutable, and setting elements of one alters the values of the other.\n\nThe new dimensions may be specified either as a list of arguments or as a shape tuple. At most one dimension may be specified with a \\texttt{:}, in which case its length is computed such that its product with all the specified dimensions is equal to the length of the original array \\texttt{A}. The total number of elements must not change.\n\n\\section{Examples}\n\\begin{verbatim}\njulia> A = Vector(1:16)\n16-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n\njulia> reshape(A, (4, 4))\n4×4 Matrix{Int64}:\n 1  5   9  13\n 2  6  10  14\n 3  7  11  15\n 4  8  12  16\n\njulia> reshape(A, 2, :)\n2×8 Matrix{Int64}:\n 1  3  5  7   9  11  13  15\n 2  4  6  8  10  12  14  16\n\njulia> reshape(1:6, 2, 3)\n2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\n 1  3  5\n 2  4  6\n\\end{verbatim}\n",
            "text/markdown": "```\nreshape(A, dims...) -> AbstractArray\nreshape(A, dims) -> AbstractArray\n```\n\nReturn an array with the same data as `A`, but with different dimension sizes or number of dimensions. The two arrays share the same underlying data, so that the result is mutable if and only if `A` is mutable, and setting elements of one alters the values of the other.\n\nThe new dimensions may be specified either as a list of arguments or as a shape tuple. At most one dimension may be specified with a `:`, in which case its length is computed such that its product with all the specified dimensions is equal to the length of the original array `A`. The total number of elements must not change.\n\n# Examples\n\n```jldoctest\njulia> A = Vector(1:16)\n16-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n\njulia> reshape(A, (4, 4))\n4×4 Matrix{Int64}:\n 1  5   9  13\n 2  6  10  14\n 3  7  11  15\n 4  8  12  16\n\njulia> reshape(A, 2, :)\n2×8 Matrix{Int64}:\n 1  3  5  7   9  11  13  15\n 2  4  6  8  10  12  14  16\n\njulia> reshape(1:6, 2, 3)\n2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\n 1  3  5\n 2  4  6\n```\n",
            "text/plain": [
              "\u001b[36m  reshape(A, dims...) -> AbstractArray\u001b[39m\n",
              "\u001b[36m  reshape(A, dims) -> AbstractArray\u001b[39m\n",
              "\n",
              "  Return an array with the same data as \u001b[36mA\u001b[39m, but with different dimension sizes\n",
              "  or number of dimensions. The two arrays share the same underlying data, so\n",
              "  that the result is mutable if and only if \u001b[36mA\u001b[39m is mutable, and setting elements\n",
              "  of one alters the values of the other.\n",
              "\n",
              "  The new dimensions may be specified either as a list of arguments or as a\n",
              "  shape tuple. At most one dimension may be specified with a \u001b[36m:\u001b[39m, in which case\n",
              "  its length is computed such that its product with all the specified\n",
              "  dimensions is equal to the length of the original array \u001b[36mA\u001b[39m. The total number\n",
              "  of elements must not change.\n",
              "\n",
              "\u001b[1m  Examples\u001b[22m\n",
              "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
              "\n",
              "\u001b[36m  julia> A = Vector(1:16)\u001b[39m\n",
              "\u001b[36m  16-element Vector{Int64}:\u001b[39m\n",
              "\u001b[36m    1\u001b[39m\n",
              "\u001b[36m    2\u001b[39m\n",
              "\u001b[36m    3\u001b[39m\n",
              "\u001b[36m    4\u001b[39m\n",
              "\u001b[36m    5\u001b[39m\n",
              "\u001b[36m    6\u001b[39m\n",
              "\u001b[36m    7\u001b[39m\n",
              "\u001b[36m    8\u001b[39m\n",
              "\u001b[36m    9\u001b[39m\n",
              "\u001b[36m   10\u001b[39m\n",
              "\u001b[36m   11\u001b[39m\n",
              "\u001b[36m   12\u001b[39m\n",
              "\u001b[36m   13\u001b[39m\n",
              "\u001b[36m   14\u001b[39m\n",
              "\u001b[36m   15\u001b[39m\n",
              "\u001b[36m   16\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> reshape(A, (4, 4))\u001b[39m\n",
              "\u001b[36m  4×4 Matrix{Int64}:\u001b[39m\n",
              "\u001b[36m   1  5   9  13\u001b[39m\n",
              "\u001b[36m   2  6  10  14\u001b[39m\n",
              "\u001b[36m   3  7  11  15\u001b[39m\n",
              "\u001b[36m   4  8  12  16\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> reshape(A, 2, :)\u001b[39m\n",
              "\u001b[36m  2×8 Matrix{Int64}:\u001b[39m\n",
              "\u001b[36m   1  3  5  7   9  11  13  15\u001b[39m\n",
              "\u001b[36m   2  4  6  8  10  12  14  16\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> reshape(1:6, 2, 3)\u001b[39m\n",
              "\u001b[36m  2×3 reshape(::UnitRange{Int64}, 2, 3) with eltype Int64:\u001b[39m\n",
              "\u001b[36m   1  3  5\u001b[39m\n",
              "\u001b[36m   2  4  6\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uhvL6swY5ur-",
        "outputId": "5ea35bc2-1b4c-446a-be0a-0ff37e5be3d5"
      },
      "source": [
        "@doc repeat"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{verbatim}\nrepeat(A::AbstractArray, counts::Integer...)\n\\end{verbatim}\nConstruct an array by repeating array \\texttt{A} a given number of times in each dimension, specified by \\texttt{counts}.\n\n\\section{Examples}\n\\begin{verbatim}\njulia> repeat([1, 2, 3], 2)\n6-element Vector{Int64}:\n 1\n 2\n 3\n 1\n 2\n 3\n\njulia> repeat([1, 2, 3], 2, 3)\n6×3 Matrix{Int64}:\n 1  1  1\n 2  2  2\n 3  3  3\n 1  1  1\n 2  2  2\n 3  3  3\n\\end{verbatim}\n\\begin{verbatim}\nrepeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\n\\end{verbatim}\nConstruct an array by repeating the entries of \\texttt{A}. The i-th element of \\texttt{inner} specifies the number of times that the individual entries of the i-th dimension of \\texttt{A} should be repeated. The i-th element of \\texttt{outer} specifies the number of times that a slice along the i-th dimension of \\texttt{A} should be repeated. If \\texttt{inner} or \\texttt{outer} are omitted, no repetition is performed.\n\n\\section{Examples}\n\\begin{verbatim}\njulia> repeat(1:2, inner=2)\n4-element Vector{Int64}:\n 1\n 1\n 2\n 2\n\njulia> repeat(1:2, outer=2)\n4-element Vector{Int64}:\n 1\n 2\n 1\n 2\n\njulia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\n4×6 Matrix{Int64}:\n 1  2  1  2  1  2\n 1  2  1  2  1  2\n 3  4  3  4  3  4\n 3  4  3  4  3  4\n\\end{verbatim}\n\\begin{verbatim}\nrepeat(s::AbstractString, r::Integer)\n\\end{verbatim}\nRepeat a string \\texttt{r} times. This can be written as \\texttt{s\\^{}r}.\n\nSee also: \\href{@ref :^(::Union{AbstractString, AbstractChar}, ::Integer)}{\\texttt{\\^{}}}\n\n\\section{Examples}\n\\begin{verbatim}\njulia> repeat(\"ha\", 3)\n\"hahaha\"\n\\end{verbatim}\n\\begin{verbatim}\nrepeat(c::AbstractChar, r::Integer) -> String\n\\end{verbatim}\nRepeat a character \\texttt{r} times. This can equivalently be accomplished by calling \\href{@ref :^(::Union{AbstractString, AbstractChar}, ::Integer)}{\\texttt{c\\^{}r}}.\n\n\\section{Examples}\n\\begin{verbatim}\njulia> repeat('A', 3)\n\"AAA\"\n\\end{verbatim}\n",
            "text/markdown": "```\nrepeat(A::AbstractArray, counts::Integer...)\n```\n\nConstruct an array by repeating array `A` a given number of times in each dimension, specified by `counts`.\n\n# Examples\n\n```jldoctest\njulia> repeat([1, 2, 3], 2)\n6-element Vector{Int64}:\n 1\n 2\n 3\n 1\n 2\n 3\n\njulia> repeat([1, 2, 3], 2, 3)\n6×3 Matrix{Int64}:\n 1  1  1\n 2  2  2\n 3  3  3\n 1  1  1\n 2  2  2\n 3  3  3\n```\n\n```\nrepeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\n```\n\nConstruct an array by repeating the entries of `A`. The i-th element of `inner` specifies the number of times that the individual entries of the i-th dimension of `A` should be repeated. The i-th element of `outer` specifies the number of times that a slice along the i-th dimension of `A` should be repeated. If `inner` or `outer` are omitted, no repetition is performed.\n\n# Examples\n\n```jldoctest\njulia> repeat(1:2, inner=2)\n4-element Vector{Int64}:\n 1\n 1\n 2\n 2\n\njulia> repeat(1:2, outer=2)\n4-element Vector{Int64}:\n 1\n 2\n 1\n 2\n\njulia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\n4×6 Matrix{Int64}:\n 1  2  1  2  1  2\n 1  2  1  2  1  2\n 3  4  3  4  3  4\n 3  4  3  4  3  4\n```\n\n```\nrepeat(s::AbstractString, r::Integer)\n```\n\nRepeat a string `r` times. This can be written as `s^r`.\n\nSee also: [`^`](@ref :^(::Union{AbstractString, AbstractChar}, ::Integer))\n\n# Examples\n\n```jldoctest\njulia> repeat(\"ha\", 3)\n\"hahaha\"\n```\n\n```\nrepeat(c::AbstractChar, r::Integer) -> String\n```\n\nRepeat a character `r` times. This can equivalently be accomplished by calling [`c^r`](@ref :^(::Union{AbstractString, AbstractChar}, ::Integer)).\n\n# Examples\n\n```jldoctest\njulia> repeat('A', 3)\n\"AAA\"\n```\n",
            "text/plain": [
              "\u001b[36m  repeat(A::AbstractArray, counts::Integer...)\u001b[39m\n",
              "\n",
              "  Construct an array by repeating array \u001b[36mA\u001b[39m a given number of times in each\n",
              "  dimension, specified by \u001b[36mcounts\u001b[39m.\n",
              "\n",
              "\u001b[1m  Examples\u001b[22m\n",
              "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
              "\n",
              "\u001b[36m  julia> repeat([1, 2, 3], 2)\u001b[39m\n",
              "\u001b[36m  6-element Vector{Int64}:\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m   3\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m   3\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> repeat([1, 2, 3], 2, 3)\u001b[39m\n",
              "\u001b[36m  6×3 Matrix{Int64}:\u001b[39m\n",
              "\u001b[36m   1  1  1\u001b[39m\n",
              "\u001b[36m   2  2  2\u001b[39m\n",
              "\u001b[36m   3  3  3\u001b[39m\n",
              "\u001b[36m   1  1  1\u001b[39m\n",
              "\u001b[36m   2  2  2\u001b[39m\n",
              "\u001b[36m   3  3  3\u001b[39m\n",
              "\n",
              "\u001b[36m  repeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\u001b[39m\n",
              "\n",
              "  Construct an array by repeating the entries of \u001b[36mA\u001b[39m. The i-th element of \u001b[36minner\u001b[39m\n",
              "  specifies the number of times that the individual entries of the i-th\n",
              "  dimension of \u001b[36mA\u001b[39m should be repeated. The i-th element of \u001b[36mouter\u001b[39m specifies the\n",
              "  number of times that a slice along the i-th dimension of \u001b[36mA\u001b[39m should be\n",
              "  repeated. If \u001b[36minner\u001b[39m or \u001b[36mouter\u001b[39m are omitted, no repetition is performed.\n",
              "\n",
              "\u001b[1m  Examples\u001b[22m\n",
              "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
              "\n",
              "\u001b[36m  julia> repeat(1:2, inner=2)\u001b[39m\n",
              "\u001b[36m  4-element Vector{Int64}:\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> repeat(1:2, outer=2)\u001b[39m\n",
              "\u001b[36m  4-element Vector{Int64}:\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m   1\u001b[39m\n",
              "\u001b[36m   2\u001b[39m\n",
              "\u001b[36m  \u001b[39m\n",
              "\u001b[36m  julia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\u001b[39m\n",
              "\u001b[36m  4×6 Matrix{Int64}:\u001b[39m\n",
              "\u001b[36m   1  2  1  2  1  2\u001b[39m\n",
              "\u001b[36m   1  2  1  2  1  2\u001b[39m\n",
              "\u001b[36m   3  4  3  4  3  4\u001b[39m\n",
              "\u001b[36m   3  4  3  4  3  4\u001b[39m\n",
              "\n",
              "\u001b[36m  repeat(s::AbstractString, r::Integer)\u001b[39m\n",
              "\n",
              "  Repeat a string \u001b[36mr\u001b[39m times. This can be written as \u001b[36ms^r\u001b[39m.\n",
              "\n",
              "  See also: \u001b[36m^\u001b[39m\n",
              "\n",
              "\u001b[1m  Examples\u001b[22m\n",
              "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
              "\n",
              "\u001b[36m  julia> repeat(\"ha\", 3)\u001b[39m\n",
              "\u001b[36m  \"hahaha\"\u001b[39m\n",
              "\n",
              "\u001b[36m  repeat(c::AbstractChar, r::Integer) -> String\u001b[39m\n",
              "\n",
              "  Repeat a character \u001b[36mr\u001b[39m times. This can equivalently be accomplished by calling\n",
              "  \u001b[36mc^r\u001b[39m.\n",
              "\n",
              "\u001b[1m  Examples\u001b[22m\n",
              "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
              "\n",
              "\u001b[36m  julia> repeat('A', 3)\u001b[39m\n",
              "\u001b[36m  \"AAA\"\u001b[39m"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "HXjS4OMSrjf7",
        "outputId": "e6d66806-dd7e-4851-c5d7-5088ac64ac95"
      },
      "source": [
        "\"\"\"\n",
        "    accuracy(model::NERTagger, data, i2t)\n",
        "    push!(output, get!(t2i, instance[l][2], 6))\n",
        "Return accuracy of tags given a model and dataset\n",
        "\"\"\"\n",
        "function accuracy(model::NERTagger, data, i2t)\n",
        "    ncorrect = 0\n",
        "    ntokens = 0\n",
        "\n",
        "    for (x, ygold, batchsizes) in data\n",
        "        scores = model(x; batchsizes=batchsizes)\n",
        "        # Your code here\n",
        "        a = vec(argmax(scores, dims=1))\n",
        "        bb = repeat( (1:size(scores,1)) , 1, size(scores,2))\n",
        "        ncorrect = ncorrect + sum(bb[a] .== ygold)\n",
        "        ntokens = ntokens + length(ygold)\n",
        "        \n",
        "        \n",
        "        if ntokens == 1\n",
        "        return ncorrect / ntokens\n",
        "        end\n",
        "\n",
        "    end\n",
        "\n",
        "    return ncorrect / ntokens\n",
        "end\n",
        "\n",
        "@info \"Testing accuracy function\"\n",
        "accuracy(model, devdata, data.i2t)\n",
        "@test accuracy(model, devdata, data.i2t) ≈ 0.0239700566564975"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Testing accuracy function\n",
            "└ @ Main In[191]:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[191]:30\u001b[22m\n",
            "  Expression: accuracy(model, devdata, data.i2t) ≈ 0.0239700566564975\n",
            "   Evaluated: 0.22206155752943607 ≈ 0.0239700566564975\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mThere was an error during testing\u001b[39m",
            "",
            "Stacktrace:",
            " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:772",
            " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)",
            "   @ Test /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:555",
            " [3] top-level scope",
            "   @ In[191]:30",
            " [4] eval",
            "   @ ./boot.jl:360 [inlined]",
            " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
            "   @ Base ./loading.jl:1094"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FP4XRN6rnOh"
      },
      "source": [
        "The following function can be used to train our model. trn is the training data, dev is used to determine the best model, tst... can be zero or more small test datasets for loss reporting. It returns the model that does best on dev."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8um9NCEDnSp0",
        "outputId": "4c14d211-2b60-45cf-8519-fb252fbe2806"
      },
      "source": [
        "\"\"\"\n",
        "    train!(model, trn, dev, tst...)\n",
        "\n",
        "Train `model` on `trn` data with Adam optimizer and Return the best performing model on `dev` data.\n",
        "\"\"\"\n",
        "function train!(model, trn, dev, tst...)\n",
        "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
        "    progress!(adam(model, trn), steps=1000) do y\n",
        "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
        "        if losses[1] < bestloss\n",
        "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
        "        end\n",
        "        return (losses...,)\n",
        "    end\n",
        "    return bestmodel\n",
        "end"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Knet.Train20.train!"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeNHFepUrrmd"
      },
      "source": [
        "## Training the model\n",
        "Here we train our model for 10 epochs using the previous procedure. You can try to fiddle with the hyperparameters i.e. (embed_size, hidden_size, epochs, etc..) to get better loss on dev set. You should get a value of dev loss around `0.26`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRdCb3fhruKM",
        "outputId": "947c0c90-a295-45b2-a6c5-ac24de2e1cc8"
      },
      "source": [
        "@info \"Training NERTagger model\"\n",
        "@info \"Seeding random number generator\"\n",
        "Random.seed!(1)\n",
        "\n",
        "@info \"Loading data\"\n",
        "data = WikiNERData();\n",
        "dtrn = WikiNERProcessed(data.trn, data.w2i, data.t2i);\n",
        "ddev = WikiNERProcessed(data.dev, data.w2i, data.t2i; shuffled=false);\n",
        "epochs = 1; @show epochs\n",
        "ctrn = [ b for b in dtrn ]\n",
        "trnx10 = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
        "trn20 = ctrn[1:20]\n",
        "dev = [ b for b in ddev ]\n",
        "\n",
        "@info \"Initializing model\"\n",
        "\n",
        "@show nwords\n",
        "@show ntags\n",
        "embed_size = 128; @show embed_size\n",
        "rnn_size = 50; @show rnn_size\n",
        "hidden_size = 32; @show hidden_size\n",
        "model = NERTagger(nwords, ntags, embed_size, rnn_size, hidden_size)\n",
        "\n",
        "# train the model (one epoch should take around 2 mins on gpu):\n",
        "model = train!(model, trnx10, dev, trn20)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Training NERTagger model\n",
            "└ @ Main In[193]:1\n",
            "┌ Info: Seeding random number generator\n",
            "└ @ Main In[193]:2\n",
            "┌ Info: Loading data\n",
            "└ @ Main In[193]:5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs = 1\n",
            "nwords = 30876\n",
            "ntags = 9\n",
            "embed_size = 128\n",
            "rnn_size = 50\n",
            "hidden_size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Initializing model\n",
            "└ @ Main In[193]:15\n",
            "┣████████████████████┫ [100.00%, 8884/8884, 02:49/02:49, 52.63i/s] (0.3259385f0, 0.18131289f0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NERTagger(Embedding(P(Knet.KnetArrays.KnetMatrix{Float32}(128,30876))), LSTM(input=128,hidden=50), Hidden(P(Knet.KnetArrays.KnetMatrix{Float32}(32,50)), P(Knet.KnetArrays.KnetMatrix{Float32}(32,1)), Knet.Ops20.relu), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(9,32)), P(Knet.KnetArrays.KnetMatrix{Float32}(9,1))))"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v18_FFcWX-1"
      },
      "source": [
        "## Evaluation of the best model\n",
        "**Expected Values**\n",
        "- Development loss = 0.25991333\n",
        "- Development accuracy = 0.9176301689440357\n",
        "- Training loss = 0.11450425\n",
        "- Training accuracy = 0.9643299845754065"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VShfji_XrzWx",
        "outputId": "aedf734e-554f-4c31-cf6c-7d9668679f4c"
      },
      "source": [
        "@info \"Evaluating the model\"\n",
        "\n",
        "dloss = loss(model, ddev)\n",
        "tloss = loss(model, dtrn)\n",
        "dacc = accuracy(model, ddev, data.i2t)\n",
        "tacc = accuracy(model, dtrn, data.i2t)\n",
        "\n",
        "println(\"Development loss = \", dloss)\n",
        "println(\"Development accuracy = \", dacc)\n",
        "println(\"Training loss = \", tloss)\n",
        "println(\"Training accuracy = \", tacc)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "┌ Info: Evaluating the model\n",
            "└ @ Main In[194]:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development loss = 0.3259385\n",
            "Development accuracy = 0.843911381945879\n",
            "Training loss = 0.18757011\n",
            "Training accuracy = 0.8719183429463659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMidUQB03vJ"
      },
      "source": [
        "Add new code cells by clicking the `+ Code` button (or _Insert_ > _Code cell_).\n",
        "\n",
        "Have fun!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-mask.png\" height=\"100\" />"
      ]
    }
  ]
}